Title: code2vec: Learning Distributed Representations of Code

Authors: Uri Alon, Meital Zilberstein, Omer Levy and Eran Yahav

Published in: POPL 2019.

Keywords: Big Code, Machine Learning, Distributed Representations.

Link: https://dl.acm.org/citation.cfm?id=3290353

-------------x-------------x-------------x-------------

[1] Problem:

Distributed representations of words, sentences, paragraphs, and documents are essentials in neural network based NLP tasks. 
The vector representation is called embeddings where the meaning is distributed through various vector components. 
Existing approaches represent code as token but using the syntax of the code is more beneficial in programming task. 
The goal of this paper is to learn the embedding from code snippet so that it can be used in programming languages task. 
The embedding of code snippet is key as similar code snippets result in similar code vectors. 
The applications of this embeddings can be code review, code retrieval, tagging, classification, clone detection, etc. 
One motivating task is labeling the semantic of code snippets: given a method body authors want to predict the method name. 
In this paper, authors present a path-context based neural attention-based model that learns code embeddings and predicts program properties i.e. code label.

[2] Solution:

The code2vec model works as (1) Take code snippet and the corresponding label as input, (2)  Extract syntactic paths 
from AST of code snippet, (3) Represent syntactic paths as a bag of distributed path-context triplets, (4) Map each 
path-context to a combined context vector by concatenation of independent triplet vectors with fully connected layer. 
(5) Aggregate multiple context vectors into a single code vector with neural attention, and (6) Use code vector for 
various programming tasks. 
The key challenge is how to present the code snippet with leveraging the semantic information that can be used to predict
the properties i.e. code label. Authors use the syntactic paths as a representation of code snippet but new challenges 
arise such that how to aggregate the extracted paths of code snippet and which path to follow for prediction. To address 
these challenges authors use neural attention approach that learns how much attention should be given to each path-context.
This aggregates each path-context into a single vector that contains the semantics of the whole code snippet. As multiple
path-context is more efficient than single path-context, authors use the soft attention over distributed syntactic paths 
instead of selecting a single path-context. And the predicted distribution of tag is computed with softmax function. 

[3] Evaluation:

Authors collected 12M methods from 10K HitHub Java repositories. They took top 1M paths and randomly sampled up to 
200 path-contexts. The dataset created by shuffled projects methods in respect of sizes and then split to training,
validation and testing set. Authors aimed to answer the following RQ: 
(1) How useful is our model in predicting method names, and how well does it measure in comparison to other recent approaches? 
(2) What is the contribution of the attention mechanism to the model? How well would it perform using hard attention instead, or using no attention at all? 
(3) What is the contribution of each of the path-context components to the model? 
(4) Is it actually able to predict names of complex methods, or only of trivial ones? 
(5) What are the properties of the learned vectors? Which semantic patterns do they encode? 
Authors use precision, recall, and F1 score as evaluation metrics. Different type of evaluation are performed: 
(1) Quantitative Evaluation: Compare code2vec with CNN+Attention, LSTM+attention, Paths+CRFs. 
(2) Alternative Design: Attention (compare soft attention with no attention, hard attention, train-soft predict-hard), removing the fully-connected layer to see the contribution of each component, element-wise soft attention. 
(3) Ablation Study: Compare full path-context with only-values, no-values, value-path, one-value. 
(4) Qualitative Evaluation: the importance of interpreting the attention and semantic properties of the learned embeddings. 
The result shows that code2vec is effective in terms of prediction metrics, generalization ability and complexity compared to previous works. 
The model achieves its best results after 30 hours for k=200 path-contexts, increasing the further time or path-context doesnâ€™t have significant improvement.

