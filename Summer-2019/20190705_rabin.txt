Title: The Adverse Effects of Code Duplication in Machine Learning Models of Code

Authors: Miltiadis Allamanis

Published in: CoRR 2018

Keywords: N/A

Link: https://arxiv.org/abs/1812.06469

-------------x-------------x-------------x-------------

[1] Problem:

Machine learning models are used to perform the learning task on the big field of code.
Recent studies show that the big code contains a significant amount of near-duplicate code.
The code duplication may negatively impact the performance of machine learning models.
But the impact of code duplication has not been by researchers.
Therefore, the authors examine the effect of code duplication on machine learning models.
The result shows the code duplication biases the evaluation in the learning task.

[2] Solution:

To measure the impact of code duplication, authors follow the following steps:
(a) Re-implement SourcererCC's token-level code duplication detection technique to detect near-duplicates.
(b) Prepare dataset with near-duplicate and deduplicate concept: 
    Fully Unbiased, Unbiased Test, Cross-set Biased Test, and Fully Biased Test.
(c) Performance evaluation on different metrics: Biased vs Unbiased, Model Capacity, and Learning Task.  

Authors report the observations and hint some recommendations for best practices.

[3] Evaluation:

The authors illustrate the impact of code duplication based on Biased vs Unbiased on the 'Autocompletion' task 
on the JavaScript-150k dataset. The computed metrcis are: ACC, ACC-ID, MRR, MRR-ID, PPL, PPL-ID. The result shows 
that all metrics are affected by code duplication but identified related metrics are affected more severely. 
The authors perform the case study of code duplication over model capacity on the NLM model with different capacity 
trained on the JavaScript-150k dataset. The result shows that models having larger capacity are affected hevaily by 
code suplication than smaller capacity. Therefore, comparison between different models having different capacity are 
not effective. Another observation made by authors for code duplication is performed over a series of tasks: 
Method Naming Model (code2vec), Variable Naming Model (JsNICE), Code Autocompletion Model (PHOG), and Docstring 
Prediction Model (seq2seq). The result shows that different models and tasks are affected differently by code duplication.
